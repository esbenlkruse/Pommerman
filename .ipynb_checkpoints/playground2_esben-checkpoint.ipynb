{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvgeQTQdqgmA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5219
    },
    "colab_type": "code",
    "id": "j42s1JgijFNo",
    "outputId": "8dc3d593-4cf3-4e13-b65b-0fc286dcf02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Pommerman'...\n",
      "remote: Enumerating objects: 109, done.\u001b[K\n",
      "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 109 (delta 10), reused 103 (delta 9), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (109/109), 1.12 MiB | 2.46 MiB/s, done.\n",
      "Resolving deltas: 100% (10/10), done.\n",
      "  File \"setup.py\", line 14\n",
      "    \"\"\".format(*CURRENT_PYTHON, *MIN_PYTHON))\n",
      "                                ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/MultiAgentLearning/playground\n",
    "!git clone https://PedFox:mads60612201@github.com/esbenlkruse/Pommerman\n",
    "\n",
    "os.chdir('Pommerman')\n",
    "os.chdir('playground')\n",
    "\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "B5EuhHukljO_",
    "outputId": "aa793f9c-6d0d-4977-a89f-ce3554764382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtorch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision --user\n",
    "import torch\n",
    "\n",
    "# install JSAnimation\n",
    "#! pip install git+https://github.com/jakevdp/JSAnimation.git\n",
    "  \n",
    "# install gym\n",
    "! pip install git+https://github.com/openai/gym --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rr2VcLfAOcDh"
   },
   "outputs": [],
   "source": [
    "try: \n",
    "  import gym\n",
    "except ImportError:\n",
    "    print(\"Please restart and run all.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "-pIDjYQ8mQQo",
    "outputId": "1c82210b-6982-4854-bdf4-2f2466dd581b"
   },
   "outputs": [],
   "source": [
    "!pom_battle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUzAcJhS03oW"
   },
   "source": [
    "# This is where the \"Pommerman Demo\" starts.\n",
    "\n",
    "This notebook demonstrates how to train Pommerman agents. Please let us know at support@pommerman.com if you run into any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pAQTcpej03oY",
    "outputId": "3dd71784-87ee-417c-819d-92ff18b2b87d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmKwD6ZT03oc"
   },
   "source": [
    "# Random agents\n",
    "\n",
    "The following codes instantiates the environment with four random agents who take actions until the game is finished. (This will be a quick game.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4yrH9Yw03oe"
   },
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uAhIWl103ol"
   },
   "outputs": [],
   "source": [
    "# Add four random agents\n",
    "agents = {}\n",
    "for agent_id in range(4):\n",
    "    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Ow_drmxG03oo",
    "outputId": "d9d74ac8-f8a0-4807-b246-792af474c276"
   },
   "outputs": [],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the random agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    #env.render()\n",
    "    actions = env.act(obs)\n",
    "    print(actions)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "#env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "#print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCWbG4JbA0re"
   },
   "outputs": [],
   "source": [
    "import pommerman.characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wQ_QfSTA2fo"
   },
   "source": [
    "# Running our own Code\n",
    "\n",
    "Trying to understand env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyQiKw9XDF6m"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nw5h6xkDB1jV"
   },
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zjju_l8ESFY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-network\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # network\n",
    "        self.out = nn.Linear(n_inputs, n_outputs, bias=False)\n",
    "        torch.nn.init.uniform_(self.out.weight, 0, 0.01)\n",
    "        # training\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "    \n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "      \n",
    "def one_hot(i, l):\n",
    "    \"\"\"One-hot encoder for the states\"\"\"\n",
    "    a = np.zeros((len(i), l))\n",
    "    #print(i, l)\n",
    "    a[range(len(i)), i] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDY9MZHaUaxf"
   },
   "outputs": [],
   "source": [
    "'''This is the base abstraction for agents in pommerman.\n",
    "All agents should inherent from this class'''\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Parent abstract Agent.\"\"\"\n",
    "\n",
    "    def __init__(self, character=Bomber):\n",
    "        self._character = character\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self._character, attr)\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def episode_end(self, reward):\n",
    "        \"\"\"This is called at the end of the episode to let the agent know that\n",
    "        the episode has ended and what is the reward.\n",
    "\n",
    "        Args:\n",
    "          reward: The single reward scalar to this agent.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def init_agent(self, id, game_type):\n",
    "        self._character = self._character(id, game_type)\n",
    "\n",
    "    @staticmethod\n",
    "    def has_user_input():\n",
    "        return False\n",
    "\n",
    "    def shutdown(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWp3iFdACTrJ"
   },
   "outputs": [],
   "source": [
    "# Add own agent\n",
    "\n",
    "class OwnAgent(BaseAgent):\n",
    "    \"\"\"The Own Agent that returns random actions given an action_space.\"\"\"\n",
    "    def __init__(self, character=Bomber):\n",
    "        self._character = character\n",
    "        self.n_inputs = 14 #env.observation_space.n\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma = 0.99 # discount rate\n",
    "\n",
    "        self.qnet = QNetwork(self.n_inputs, self.n_outputs, self.learning_rate)\n",
    "        \n",
    "        self.Q = 0\n",
    "    \n",
    "    def act(self, obs, action_space):\n",
    "        # 1. do foward pass of current state to compute Q-values for all actions\n",
    "        self.qnet.optimizer.zero_grad()\n",
    "        #print(obs)\n",
    "        self.Q = self.qnet(torch.from_numpy(one_hot(obs['board'].flatten(), self.n_inputs)).float())\n",
    "\n",
    "        # 2. select action with epsilon-greedy strategy\n",
    "        a = self.Q.argmax().item() if np.random.rand() > epsilon else env.action_space.sample()\n",
    "        \n",
    "        return a\n",
    "      \n",
    "    def afterstep(self, s1, r, a):\n",
    "        # 3. do forward pass for the next state\n",
    "        with torch.no_grad():\n",
    "            Q1 = self.qnet(torch.from_numpy( one_hot(s1['board'].flatten(), self.n_inputs)).float())\n",
    "\n",
    "        # 4. set Q-target\n",
    "        q_target = self.Q.clone()\n",
    "        q_target[0, a] = r + self.gamma * Q1.max().item() * (not done)\n",
    "\n",
    "        # 5. update network weights\n",
    "        loss = self.qnet.loss(self.Q, q_target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.qnet.optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKRUQoEwB2gS"
   },
   "outputs": [],
   "source": [
    "# Add four random agents\n",
    "agents = {}\n",
    "\n",
    "for agent_id in range(3):\n",
    "    agents[agent_id] = SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "    \n",
    "agents[3] = OwnAgent(config[\"agent\"](3, config[\"game_type\"]))\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zg9MbFMA5mG"
   },
   "outputs": [],
   "source": [
    "## Seed and reset the environment\n",
    "#env.seed(0)\n",
    "#obs = env.reset()\n",
    "#\n",
    "## Run the random agents until we're done\n",
    "#done = False\n",
    "#while not done:\n",
    "#    #env.render()\n",
    "#    actions = env.act(obs)\n",
    "#    obs, reward, done, info = env.step(actions)\n",
    "##env.render(close=True)\n",
    "#env.close()\n",
    "#\n",
    "#print(info)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nG_oAHcMEiG7",
    "outputId": "f3daf416-8363-4d4a-f6e2-a75771ebb09d"
   },
   "outputs": [],
   "source": [
    "# train Q-network\n",
    "\n",
    "num_episodes = 1000\n",
    "episode_limit = 100\n",
    "\n",
    "val_freq = 100 # validation frequency\n",
    "\n",
    "try:\n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    print('start training')\n",
    "    for i in range(num_episodes):\n",
    "        # init new episode\n",
    "        s, ep_reward, ep_loss = env.reset(), 0, 0\n",
    "        \n",
    "        for j in range(episode_limit):\n",
    "            \n",
    "            actions = env.act(s)\n",
    "            print(\"\",end=\"\")\n",
    "            s1, r_tot, done, _ = env.step(actions)\n",
    "            r = r_tot[3]\n",
    "            \n",
    "            loss = agents[3].afterstep(s1[3],r, actions[3])\n",
    "            \n",
    "            # 6. bookkeeping\n",
    "            s = s1\n",
    "        \n",
    "            ep_loss += loss.item()\n",
    "            if done: break\n",
    "        \n",
    "        ep_reward += r\n",
    "        \n",
    "        # bookkeeping\n",
    "        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if (i+1) % val_freq == 0: print('{:5d} mean training reward: {:5.2f}'.format(i+1, np.mean(rewards[-val_freq:])))\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "colab_type": "code",
    "id": "Yi_ozobFBx6I",
    "outputId": "07fa9836-2bd7-428b-a33a-57043ae50cd0"
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(411)\n",
    "plt.title('training rewards')\n",
    "plt.plot(range(1, num_episodes+1), rewards)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(rewards))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(412)\n",
    "plt.title('training lengths')\n",
    "plt.plot(range(1, num_episodes+1), lengths)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(lengths))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(413)\n",
    "plt.title('training loss')\n",
    "plt.plot(range(1, num_episodes+1), losses)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(losses))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(414)\n",
    "plt.title('epsilon')\n",
    "plt.plot(range(1, num_episodes+1), epsilons)\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "playground2_esben.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
